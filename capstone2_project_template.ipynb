{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CAPSTONE PROJECT 2 TEMPLATE**\n",
    "### [Your Project Title Here]\n",
    "\n",
    "**Student Name:** [Your Name]  \n",
    "**Date:** [Date]  \n",
    "**Course:** Intermediate AI & Data Science  \n",
    "**Instructor:** Amir Charkhi  \n",
    "**AI Tech Institute**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Project Overview\n",
    "\n",
    "**Problem:** [One sentence describing what you're predicting/forecasting]\n",
    "\n",
    "**Business Value:** [Why this matters - 1-2 sentences]\n",
    "\n",
    "**Data Source:** [Where you got the data]\n",
    "\n",
    "**Target Variable:** [What you're predicting]\n",
    "\n",
    "**Success Metric:** [How you'll measure success]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly for interactive visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Add your specific model imports here\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics (adjust based on your problem type)\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,  # Classification\n",
    "    mean_squared_error, mean_absolute_error, r2_score,        # Regression\n",
    "    classification_report, confusion_matrix                    # Detailed reports\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model persistence\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# OR\n",
    "# df = pd.read_excel('your_data.xlsx')\n",
    "# OR\n",
    "# Load from Kaggle, API, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "# f\"Dataset: {df.shape[0]} rows Ã— {df.shape[1]} columns\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 3.1 Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "# missing = df.isnull().sum()\n",
    "# missing[missing > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze your target variable\n",
    "# For classification: distribution of classes\n",
    "# For regression: distribution of values\n",
    "# For time series: plot over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of key features\n",
    "# Use histograms, box plots, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "# numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "# corr = df[numerical_cols].corr()\n",
    "\n",
    "# fig = px.imshow(corr, \n",
    "#                 text_auto='.2f',\n",
    "#                 title='Feature Correlations')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Key Insights from EDA\n",
    "\n",
    "**Document your findings here:**\n",
    "- Finding 1: [What you discovered]\n",
    "- Finding 2: [What you discovered]\n",
    "- Finding 3: [What you discovered]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "### 4.1 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# Options: drop, fill with mean/median, forward fill, etc.\n",
    "# df_clean = df.dropna()\n",
    "# OR\n",
    "# df_clean = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and handle outliers if necessary\n",
    "# Use IQR method, z-score, or domain knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features if beneficial\n",
    "# Examples:\n",
    "# - Combine existing features\n",
    "# - Extract date components (year, month, day)\n",
    "# - Create ratios or differences\n",
    "# - Bin continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "# One-hot encoding for nominal categories\n",
    "# Label encoding for ordinal categories\n",
    "\n",
    "# categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "# df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "# X = df_encoded.drop('target_column', axis=1)\n",
    "# y = df_encoded['target_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shapes\n",
    "# X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify split\n",
    "# f\"Training: {X_train.shape[0]} samples | Testing: {X_test.shape[0]} samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Modeling\n",
    "\n",
    "### 6.1 Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a simple baseline model\n",
    "# Example for classification:\n",
    "# baseline_pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('model', LogisticRegression(random_state=42))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline\n",
    "# baseline_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline\n",
    "# y_pred_baseline = baseline_pipeline.predict(X_test)\n",
    "# baseline_score = accuracy_score(y_test, y_pred_baseline)\n",
    "# f\"Baseline Accuracy: {baseline_score:.1%}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model 2: [Your Second Model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a second model (e.g., Random Forest)\n",
    "# model2 = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('model', RandomForestClassifier(random_state=42))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "# y_pred_2 = model2.predict(X_test)\n",
    "# score_2 = accuracy_score(y_test, y_pred_2)\n",
    "# f\"Model 2 Accuracy: {score_2:.1%}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Model 3: [Your Third Model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a third model\n",
    "# Continue pattern from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "# results = pd.DataFrame({\n",
    "#     'Model': ['Baseline', 'Model 2', 'Model 3'],\n",
    "#     'Accuracy': [baseline_score, score_2, score_3],\n",
    "#     'Precision': [...],\n",
    "#     'Recall': [...],\n",
    "#     'F1-Score': [...]\n",
    "# })\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "# fig = px.bar(results, x='Model', y='Accuracy', \n",
    "#              title='Model Performance Comparison')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Hyperparameter Tuning\n",
    "\n",
    "### 7.1 Select Best Model for Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on comparison, select best model to tune\n",
    "# best_model = model2  # Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Define Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to search\n",
    "# param_grid = {\n",
    "#     'model__n_estimators': [50, 100, 200],\n",
    "#     'model__max_depth': [5, 10, 20, None],\n",
    "#     'model__min_samples_split': [2, 5, 10]\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "# grid_search = GridSearchCV(\n",
    "#     best_model,\n",
    "#     param_grid,\n",
    "#     cv=5,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "# grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score\n",
    "# grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "# final_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Final Evaluation\n",
    "\n",
    "### 8.1 Test Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "# y_pred_final = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "# For classification:\n",
    "# final_accuracy = accuracy_score(y_test, y_pred_final)\n",
    "# final_precision = precision_score(y_test, y_pred_final, average='weighted')\n",
    "# final_recall = recall_score(y_test, y_pred_final, average='weighted')\n",
    "# final_f1 = f1_score(y_test, y_pred_final, average='weighted')\n",
    "\n",
    "# For regression:\n",
    "# final_mse = mean_squared_error(y_test, y_pred_final)\n",
    "# final_rmse = np.sqrt(final_mse)\n",
    "# final_mae = mean_absolute_error(y_test, y_pred_final)\n",
    "# final_r2 = r2_score(y_test, y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics\n",
    "# print(f\"Final Model Performance:\")\n",
    "# print(f\"Accuracy: {final_accuracy:.1%}\")\n",
    "# print(f\"Precision: {final_precision:.1%}\")\n",
    "# print(f\"Recall: {final_recall:.1%}\")\n",
    "# print(f\"F1-Score: {final_f1:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "# print(classification_report(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification problems\n",
    "# cm = confusion_matrix(y_test, y_pred_final)\n",
    "# fig = px.imshow(cm, \n",
    "#                 text_auto=True,\n",
    "#                 title='Confusion Matrix',\n",
    "#                 labels=dict(x='Predicted', y='Actual'))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your model supports feature importance\n",
    "# importances = final_model.named_steps['model'].feature_importances_\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'Feature': X.columns,\n",
    "#     'Importance': importances\n",
    "# }).sort_values('Importance', ascending=False)\n",
    "\n",
    "# fig = px.bar(feature_importance.head(10), \n",
    "#              x='Importance', y='Feature',\n",
    "#              title='Top 10 Most Important Features',\n",
    "#              orientation='h')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Business Insights & Recommendations\n",
    "\n",
    "### 9.1 Key Findings\n",
    "\n",
    "**Finding 1:** [Your insight]\n",
    "- [Supporting detail]\n",
    "- [Business implication]\n",
    "\n",
    "**Finding 2:** [Your insight]\n",
    "- [Supporting detail]\n",
    "- [Business implication]\n",
    "\n",
    "**Finding 3:** [Your insight]\n",
    "- [Supporting detail]\n",
    "- [Business implication]\n",
    "\n",
    "### 9.2 Business Recommendations\n",
    "\n",
    "1. **Recommendation 1:** [What should the business do?]\n",
    "   - Expected impact: [Quantify if possible]\n",
    "   \n",
    "2. **Recommendation 2:** [What should the business do?]\n",
    "   - Expected impact: [Quantify if possible]\n",
    "   \n",
    "3. **Recommendation 3:** [What should the business do?]\n",
    "   - Expected impact: [Quantify if possible]\n",
    "\n",
    "### 9.3 Model Limitations\n",
    "\n",
    "- **Limitation 1:** [What doesn't the model handle well?]\n",
    "- **Limitation 2:** [What assumptions were made?]\n",
    "- **Limitation 3:** [What could go wrong?]\n",
    "\n",
    "### 9.4 Future Improvements\n",
    "\n",
    "- **Improvement 1:** [How could the model be enhanced?]\n",
    "- **Improvement 2:** [What additional data would help?]\n",
    "- **Improvement 3:** [What alternative approaches to try?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "# joblib.dump(final_model, 'final_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model metadata\n",
    "# import json\n",
    "# metadata = {\n",
    "#     'model_type': 'RandomForestClassifier',\n",
    "#     'accuracy': float(final_accuracy),\n",
    "#     'features': X.columns.tolist(),\n",
    "#     'training_date': '2024-12-XX',\n",
    "#     'best_params': grid_search.best_params_\n",
    "# }\n",
    "\n",
    "# with open('model_metadata.json', 'w') as f:\n",
    "#     json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Conclusion\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Problem:** [Restate the problem]\n",
    "\n",
    "**Solution:** [Summarize your approach]\n",
    "\n",
    "**Results:** [Key metrics and outcomes]\n",
    "\n",
    "**Impact:** [Business value delivered]\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. [What should happen next?]\n",
    "2. [How should the model be deployed?]\n",
    "3. [How should performance be monitored?]\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you!**\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
